#!/bin/bash
# Input: lpp_cn_regressors.csv generated by textgrid_cn.sh with word, onset, offset, section number
# Output: lpp_cn_regressors.csv with 5 columns: word, onset, offset, section number, subtlex frequency

# convert the subtlex_cn excel file (download: http://crr.ugent.be/programs-data/subtitle-frequencies/subtlex-ch) to csv 
# easy_install xlsx2csv
# keep only 2 decimals for word frequency
# xlsx2csv subtlex_cn.xlsx | awk -F, '{printf "%s,%.2f\n", $1, $2}' > subtlex_cn.csv
xlsx2csv subtlex_cn_wf.xlsx | awk -F, -v OFS="\t" 'NR>3 {print $1, $2}' > subtlex_cn.csv

# parse googlengram & rename files
for f in google_1gram_cn/googlebooks-chi-sim-all-1gram-20120701-?.txt; do
	awk -f googlengram.awk $f >  ${f//*-/1gram}
done

cat 1gram* > 1gram_cn.csv
rm 1gram*.txt

# sum up duplicates
sort -k 1,1 1gram_cn.csv | awk 'NR==1 {word=$1;count=$2;next} ($1==word) {count+=$2;next} \
{print word,count; word=$1;count=$2} END {print word,count}' | sort -nrk 2,2 > tmp && mv tmp 1gram_cn.csv

# concatenate subtlex and google frequency & sum up duplicates
cat subtlex_cn.csv 1gram_cn.csv | sort -k 1,1 | awk 'NR==1 {word=$1;count=$2;next} ($1==word) \
{count+=$2;next} {print word,count; word=$1;count=$2} END {print word,count}' | sort -nrk 2,2 \
> subtlex_google_cn.csv

# find subtlex+google frequency for lpp_cn words
awk -v OFS="," 'FNR==NR{a[$1]=$1;b[$1]=$2;next} ($1 in a) {print $0, b[$1];next} {print $0, 0}' \
subtlex_google_cn.csv FS="," lpp_cn_regressors.csv > tmp && mv tmp lpp_cn_regressors.csv